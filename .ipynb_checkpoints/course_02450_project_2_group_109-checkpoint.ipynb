{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f64682f",
   "metadata": {},
   "source": [
    "# Project_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1df30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eeb593e8",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0223b1",
   "metadata": {},
   "source": [
    "### Part A Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296aecb4",
   "metadata": {},
   "source": [
    "Description:\n",
    "We will try to predict \"adiposity\" which refers to the body fat percentage.\n",
    "We choose this because we have indicators that the other attributes will be able to predict it (check correlations-> high numbers).\n",
    "Also, it follows a normal distribution so we will not have troubles with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a27987",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4055b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2554974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://hastie.su.domains/ElemStatLearn/datasets/SAheart.data?fbclid=IwAR0bnadUy7l7_jwPgJzAW1Dg5RM_JyAKv_doOWxuP2Fx2XpkTAliWHRl73U') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37114ca5",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1d2bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sbp', 'tobacco', 'ldl', 'adiposity', 'typea', 'obesity', 'alcohol',\n",
      "       'age', 'chd', 'famhist_Absent', 'famhist_Present'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# drop unnecessary name variable\n",
    "data.drop(columns = 'row.names',inplace=True)\n",
    "# one hot enc of categorical variable famhist\n",
    "df_famhist = pd.get_dummies(data['famhist'],prefix=\"famhist\")\n",
    "data.drop(columns = 'famhist',inplace=True)\n",
    "data = pd.concat([data, df_famhist], axis=1)\n",
    "# !!!! In case we include chd we are going to one hot encode that too!\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6817724",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,[0,1,2,4,5,6,7,8,9,10]]\n",
    "X_data_columns = X.columns\n",
    "y = data.iloc[:,[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc88e841",
   "metadata": {},
   "source": [
    "##### Standardize the data (mean=0, std=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ddac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0738c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)\n",
    "#X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89e3ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e681d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ed08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26c02ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "CV = model_selection.KFold(K, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51a6326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge  # it's linear regression with l2 regularization\n",
    "#from sklearn.linear_model import RidgeCV  # it's linear regression with l2 regularization CV stands for cross validation\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95341ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generalization_err_per_model = {}\n",
    "training_err_per_model = {}\n",
    "lambdas = np.power(10.,range(-5,9))\n",
    "#lambdas= np.append(lambdas,[2,4,6,8])\n",
    "lambdas = np.sort(lambdas)\n",
    "\n",
    "# train_data_x, train_data_y, test_data_x, test_data_y, test_data, lambda_value, model, train_error, test_error\n",
    "models_info = []\n",
    "\n",
    "for lambda_value in lambdas:\n",
    "    generalization_err_per_model[lambda_value]=[]\n",
    "    training_err_per_model[lambda_value]=[]\n",
    "\n",
    "for train_index, test_index in CV.split(X,y):\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    for lambda_value in lambdas:\n",
    "        model = Ridge(alpha=lambda_value)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        generalization_err = mean_squared_error(y_test, y_pred, squared=True)\n",
    "        generalization_err_per_model[lambda_value].append(generalization_err)\n",
    "\n",
    "        y_pred = model.predict(X_train)\n",
    "        training_err = mean_squared_error(y_train, y_pred, squared=True)\n",
    "        training_err_per_model[lambda_value].append(mean_squared_error(y_train, y_pred, squared=True))\n",
    "        models_info.append([X_train,y_train,X_test,y_test,lambda_value,model,training_err,generalization_err])\n",
    "\n",
    "# avg_generalization_err_per_lambda_value = {}\n",
    "# for lambda_value, errors in generalization_err_per_model.items():\n",
    "#     avg_generalization_err_per_lambda_value[lambda_value] = sum(errors)/len(errors)\n",
    "# avg_generalization_err_per_lambda_value = list(avg_generalization_err_per_lambda_value.values())\n",
    "\n",
    "# avg_training_err_per_lambda_value = {}\n",
    "# for lambda_value, errors in training_err_per_model.items():\n",
    "#     avg_training_err_per_lambda_value[lambda_value] = sum(errors)/len(errors)\n",
    "# avg_training_err_per_lambda_value = list(avg_training_err_per_lambda_value.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b9955a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 39.66642282767798), (4, 34.94085311591199), (8, 43.91045536387897), (0, 31.471663172711963), (2, 25.795602102502027), (9, 41.689239968618196), (1, 32.63780357387075), (6, 17.633441648993628), (9, 33.84016855919995), (8, 34.94473955071517), (0, 32.348178823612834), (2, 27.837899317831567), (6, 39.772342857449345), (0, 35.124238729983986)]\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "#Find best labdba_value based on average generalazation error\n",
    "# list of K elements where each element is a list of K model info with same lambda_value\n",
    "models_grouped_by_lambda_value = [models_info[i:i + K] for i in range(0, len(models_info), K)]\n",
    "\n",
    "# list of K elements where each element is  a tuple with (argmin of model with min gen_err, avg_gen_err so that we can find the best avg gen_err)\n",
    "argmin_and_avg_gen_error = [ (np.argmin([model[7] for model in model_group]),np.average([model[7] for model in model_group])) for model_group in models_grouped_by_lambda_value]\n",
    "print(argmin_and_avg_gen_error)\n",
    "# index of the minimum avg_gen_err so that we can pick best lambda\n",
    "argmin_of_gen_errors = np.argmin(np.array(argmin_and_avg_gen_error)[:,1])\n",
    "optimal_lambda = lambdas[argmin_of_gen_errors]\n",
    "print(optimal_lambda)\n",
    "# average_generalazation_errors = [sum(model_group)/len(model_group) for  model_group in models_grouped_by_lambda_value]\n",
    "# #Find best model\n",
    "# best_model_info = models_info[np.argmin([model_info[7] for model_info in models_info])]\n",
    "# print([model_info[7] for model_info in models_info][4:])\n",
    "# print(best_model_info[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607698a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9011a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3bb15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65fd5779",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_training_err_per_lambda_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogx(lambdas, \u001b[43mavg_training_err_per_lambda_value\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, marker \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogx(lambdas, avg_generalization_err_per_lambda_value, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m, marker \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower right\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_training_err_per_lambda_value' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogx(lambdas, avg_training_err_per_lambda_value, label=\"Train\", marker = \".\")\n",
    "plt.semilogx(lambdas, argmin_and_avg_gen_error, label=\"Test\", marker = \".\" )\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Regularization parameter\")\n",
    "plt.ylabel(\"ASME\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b984288",
   "metadata": {},
   "source": [
    "Below he have the coefficients of the attriutes that we are using.\n",
    "Keep in mind if you see an extra coef then it is just the interecept that I might decided to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e68f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge(alpha=1.0)\n",
    "reg.fit(X_train, y_train)\n",
    "print(reg.coef_[0])\n",
    "print(list(X_data_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8971d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
